(Topicos_IA) d:\Cursos de Universidad\Semestre 9\Top IA\Dataset\Scripts>python main.py
True
0
<torch.cuda.device object at 0x000001A0A9357850>
1
NVIDIA GeForce RTX 3060 Laptop GPU
True
xlmroberta
xlm-roberta-base
Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\simpletransformers\classification\classification_model.py:345: UserWarning: use_multiprocessing automatically disabled as xlmroberta fails when using multiprocessing for feature conversion.
  warnings.warn(
D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epochs 0/5. Running Loss:    0.6325:   0%|                                                                                                                                    | 1/8750 [00:05<12:23:08,  5.10s/it]
Epoch 1 of 5:   0%|                                                                                                                                                                         | 0/5 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 161, in <module>
    train_model(model_type, model_name, train_df, test_df, num_labels)
  File "main.py", line 126, in train_model
    model.train_model(train_df,
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\simpletransformers\classification\classification_model.py", line 463, in train_model
    global_step, training_details = self.train(
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\simpletransformers\classification\classification_model.py", line 700, in train
    outputs = model(**inputs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\simpletransformers\classification\transformer_models\roberta_model.py", line 63, in forward
    outputs = self.roberta(
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 853, in forward
    encoder_outputs = self.encoder(
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 527, in forward
    layer_outputs = layer_module(
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 454, in forward
    layer_output = apply_chunking_to_forward(
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\pytorch_utils.py", line 246, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 466, in feed_forward_chunk
    intermediate_output = self.intermediate(attention_output)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\models\roberta\modeling_roberta.py", line 365, in forward
    hidden_states = self.intermediate_act_fn(hidden_states)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\modules\module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\transformers\activations.py", line 57, in forward
    return self.act(input)
  File "D:\Anaconda\Anaconda\envs\Topicos_IA\lib\site-packages\torch\nn\functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 6.00 GiB total capacity; 5.23 GiB already allocated; 0 bytes free; 5.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF